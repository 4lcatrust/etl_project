{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pendulum\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "from airflow_clickhouse_plugin.hooks.clickhouse import ClickHouseHook\n",
    "from airflow.models import Variable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull, length, current_timestamp, current_date, date_diff, round, sum as spark_sum, lit, regexp_replace, trim, expr, count_distinct\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name: str, additional_jars: list = None):\n",
    "    \"\"\"Create a SparkSession with necessary configurations\"\"\"\n",
    "    import os\n",
    "\n",
    "    active_session = SparkSession.getActiveSession()\n",
    "    if active_session:\n",
    "        active_session.stop()\n",
    "\n",
    "    # Ensure ClickHouse JDBC JAR is always included\n",
    "    clickhouse_jar = os.path.abspath(\"driver/clickhouse-jdbc-0.4.6.jar\")\n",
    "    postgres_jar = os.path.abspath(\"driver/postgresql-42.7.5.jar\")\n",
    "\n",
    "    default_jars = [clickhouse_jar, postgres_jar]\n",
    "\n",
    "    if additional_jars:\n",
    "        jars = \",\".join(default_jars + additional_jars)\n",
    "    else:\n",
    "        jars = \",\".join(default_jars)\n",
    "\n",
    "    logging.info(f\"Adding JARs to Spark session: {jars}\")\n",
    "\n",
    "    builder = SparkSession.builder.appName(app_name) \\\n",
    "        .config(\"spark.jars\", jars) \\\n",
    "        .config(\"spark.driver.extraClassPath\", jars) \\\n",
    "        .config(\"spark.executor.extraClassPath\", jars) \\\n",
    "        .config(\"spark.driver.userClassPathFirst\", \"true\") \\\n",
    "        .config(\"spark.executor.userClassPathFirst\", \"true\")\n",
    "\n",
    "    return builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG default arguments\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': pendulum.today().add(days = -1),\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': pendulum.duration(minutes = 5)\n",
    "}\n",
    "\n",
    "current_date_str = pendulum.today().to_date_string()\n",
    "\n",
    "table_source = [\n",
    "    (\"public\", \"fct_transactions\"),\n",
    "    (\"public\", \"dim_item\"),\n",
    "    (\"public\", \"dim_time\")\n",
    "]\n",
    "\n",
    "expected_schema = {\n",
    "    \"fct_transactions\" : {\n",
    "        \"payment_key\" : StringType(),\n",
    "        \"customer_key\" : StringType(),\n",
    "        \"time_key\" : StringType(),\n",
    "        \"item_key\" : StringType(),\n",
    "        \"store_key\" : StringType(),\n",
    "        \"quantity\" : IntegerType(),\n",
    "        \"unit\" : StringType(),\n",
    "        \"unit_price\" : IntegerType(),\n",
    "        \"total_price\" : IntegerType()\n",
    "    },\n",
    "    \"dim_item\" : {\n",
    "        \"item_key\" : StringType(),\n",
    "        \"item_name\" : StringType(),\n",
    "        \"desc\" : StringType(),\n",
    "        \"unit_price\" : FloatType(),\n",
    "        \"man_country\" : StringType(),\n",
    "        \"supplier\" : StringType(),\n",
    "        \"unit\" : StringType()\n",
    "    },\n",
    "    \"dim_time\" : {\n",
    "        \"time_key\" : StringType(),\n",
    "        \"date\" : StringType(),\n",
    "        \"hour\" : IntegerType(),\n",
    "        \"day\" : IntegerType(),\n",
    "        \"week\" : StringType(),\n",
    "        \"month\" : IntegerType(),\n",
    "        \"quarter\" : StringType(),\n",
    "        \"year\" : IntegerType()\n",
    "    }\n",
    "}\n",
    "\n",
    "def stg_dq_checks(df, tablename):\n",
    "    actual_schema = dict([(field.name, str(field.dataType)) for field in df.schema.fields])\n",
    "    expected_table_schema = expected_schema[tablename]\n",
    "    quality_metrics = {}\n",
    "    schema_validity = int(set(expected_table_schema.keys()) == set(actual_schema.keys())) * 100\n",
    "    quality_metrics[f\"{tablename}_schema_validity\"] = schema_validity\n",
    "    if tablename == \"fct_transactions\":\n",
    "        checks = [\n",
    "            \"customer_key\",\n",
    "            \"item_key\",\n",
    "            \"time_key\",\n",
    "            \"quantity\",\n",
    "            \"unit_price\",\n",
    "            \"total_price\"\n",
    "            ]\n",
    "        for check in checks:\n",
    "            quality_metrics[f\"{tablename}_null_{check}\"] = (\n",
    "                df.filter(col(check).isNotNull()).count() / df.count() * 100 if df.count() > 0 else 0\n",
    "            )\n",
    "        checks_negative = [\n",
    "            \"quantity\",\n",
    "            \"unit_price\",\n",
    "            \"total_price\"]\n",
    "        for check in checks_negative:\n",
    "            quality_metrics[f\"{tablename}_negative_{check}\"] = (\n",
    "                df.filter(col(check) > 0).count() / df.count() * 100 if df.count() > 0 else 0\n",
    "            )\n",
    "    elif tablename == \"dim_item\":\n",
    "        checks = [\n",
    "            \"item_key\",\n",
    "            \"desc\",\n",
    "            \"item_name\",\n",
    "            \"unit_price\"]\n",
    "        for check in checks:\n",
    "            quality_metrics[f\"{tablename}_null_{check}\"] = (\n",
    "                df.filter(col(check).isNotNull()).count() / df.count() * 100 if df.count() > 0 else 0\n",
    "            )\n",
    "        quality_metrics[f\"{tablename}_negative_unit_price\"] = (\n",
    "            df.filter(col(\"unit_price\") > 0).count() / df.count() * 100 if df.count() > 0 else 0\n",
    "        )\n",
    "    elif tablename == \"dim_time\":\n",
    "        quality_metrics[f\"{tablename}_null_time_key\"] = (\n",
    "            df.filter(col(\"time_key\").isNotNull()).count() / df.count() * 100 if df.count() > 0 else 0\n",
    "        )\n",
    "    return quality_metrics\n",
    "\n",
    "def extract(table_source : dict):\n",
    "    \"\"\"\n",
    "    Extract data from PostgreSQL and perform initial quality checks\n",
    "    \"\"\"\n",
    "    spark = create_spark_session('PostgreSQL-Extract')\n",
    "    dataframes = {}\n",
    "    for schemaname, tablename in table_source:\n",
    "        df = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", Variable.get(\"POSTGRES_JDBC_URL\")) \\\n",
    "            .option(\"dbtable\", f\"{schemaname}.{tablename}\") \\\n",
    "            .option(\"user\", \"spark\") \\\n",
    "            .option(\"password\", Variable.get(\"POSTGRES_PASSWORD\")) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load()\n",
    "        dataframes[tablename] = df\n",
    "\n",
    "    all_quality_metrics = {}\n",
    "    for tablename, df in dataframes.items():\n",
    "        quality_results = stg_dq_checks(df, tablename)\n",
    "        all_quality_metrics[tablename] = quality_results\n",
    "\n",
    "    with open(f'dq_metrics/{current_date_str}_quality_metrics.json', 'w') as f:\n",
    "        json.dump(all_quality_metrics, f)\n",
    "        f.close()\n",
    "\n",
    "    failed_checks = {}\n",
    "    for table, metrics in all_quality_metrics.items():\n",
    "        failed_metrics = {metric: val for metric, val in metrics.items() if val < 90}\n",
    "\n",
    "        if failed_metrics:\n",
    "            failed_checks[table] = failed_metrics\n",
    "\n",
    "    if failed_checks:\n",
    "        logger.error(\"Data Quality Check Failed for the following tables:\")\n",
    "        for table, metrics in failed_checks.items():\n",
    "            for metric, val in metrics.items():\n",
    "                logger.error(f\"{table}.{metric} = {val}% (Expected â‰¥ 90%)\")\n",
    "        raise Exception(f\"Data Quality Check Failed: {json.dumps(failed_checks, indent=4)}\")\n",
    "    else:\n",
    "        logger.info(\"All Data Quality Checks Passed!\")\n",
    "        for schemaname, tablename in table_source:\n",
    "            staging_path_write = f\"staging/{current_date_str}.{schemaname}.{tablename}.parquet\"\n",
    "            dataframes[tablename].write.parquet(staging_path_write, mode=\"overwrite\")\n",
    "    \n",
    "    return all_quality_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-03-01T16:18:47.129+0700\u001b[0m] {\u001b[34m1582760252.py:\u001b[0m20} INFO\u001b[0m - Adding JARs to Spark session: /Users/sawitpro/Documents/snippets/porto/etl_project/driver/clickhouse-jdbc-0.4.6.jar,/Users/sawitpro/Documents/snippets/porto/etl_project/driver/postgresql-42.7.5.jar\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-03-01T16:18:54.514+0700\u001b[0m] {\u001b[34m2371406242.py:\u001b[0m146} INFO\u001b[0m - All Data Quality Checks Passed!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fct_transactions': {'fct_transactions_schema_validity': 100,\n",
       "  'fct_transactions_null_customer_key': 100.0,\n",
       "  'fct_transactions_null_item_key': 100.0,\n",
       "  'fct_transactions_null_time_key': 100.0,\n",
       "  'fct_transactions_null_quantity': 100.0,\n",
       "  'fct_transactions_null_unit_price': 100.0,\n",
       "  'fct_transactions_null_total_price': 100.0,\n",
       "  'fct_transactions_negative_quantity': 100.0,\n",
       "  'fct_transactions_negative_unit_price': 100.0,\n",
       "  'fct_transactions_negative_total_price': 100.0},\n",
       " 'dim_item': {'dim_item_schema_validity': 100,\n",
       "  'dim_item_null_item_key': 100.0,\n",
       "  'dim_item_null_desc': 100.0,\n",
       "  'dim_item_null_item_name': 100.0,\n",
       "  'dim_item_null_unit_price': 100.0,\n",
       "  'dim_item_negative_unit_price': 100.0},\n",
       " 'dim_time': {'dim_time_schema_validity': 100,\n",
       "  'dim_time_null_time_key': 100.0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract(table_source = table_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(table_source : dict):\n",
    "    \"\"\"\n",
    "    Transform data using PySpark\n",
    "    \"\"\"\n",
    "    spark = create_spark_session(\"Spark-Transform\")\n",
    "    dataframes = {}\n",
    "    for schemaname, tablename in table_source:\n",
    "        staging_path_write = f\"staging/{current_date_str}.{schemaname}.{tablename}.parquet\"\n",
    "        df = spark.read \\\n",
    "            .format(\"parquet\") \\\n",
    "            .load(staging_path_write)\n",
    "        dataframes[tablename] = df\n",
    "\n",
    "    fct_transactions = dataframes[\"fct_transactions\"].alias(\"ft\")\n",
    "    dim_item = dataframes[\"dim_item\"].alias(\"di\")\n",
    "    dim_time = dataframes[\"dim_time\"].alias(\"dt\")\n",
    "\n",
    "    cte_df = (\n",
    "        fct_transactions\n",
    "            .join(dim_item, col(\"ft.item_key\") == col(\"di.item_key\"), \"left\")\n",
    "            .join(dim_time, col(\"ft.time_key\") == col(\"dt.time_key\"), \"left\")\n",
    "            .selectExpr(\n",
    "                \"MAKE_DATE(year, month, day) AS transaction_date\",\n",
    "                \"quantity\",\n",
    "                \"total_price\",\n",
    "                \"customer_key\",\n",
    "                \"REGEXP_REPLACE(REGEXP_REPLACE(TRIM(desc), '^[a-z]. ', ''), ' - ', ' ') AS item_category\"\n",
    "            )\n",
    "    )\n",
    "\n",
    "    transformed_df = (\n",
    "        cte_df\n",
    "        .groupBy(\"transaction_date\", \"item_category\")\n",
    "        .agg(\n",
    "            spark_sum(\"total_price\").alias(\"total_transaction_value\"),\n",
    "            spark_sum(\"quantity\").alias(\"total_goods_sold\"),\n",
    "            count_distinct(\"customer_key\").alias(\"count_transacting_customer\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    transformed_path = f\"transformed/{current_date_str}.daily_transaction_summary.parquet\"\n",
    "    transformed_df.write.parquet(transformed_path, mode=\"overwrite\")\n",
    "    logger.info(f\"Transformed data succesfully saved to: {transformed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-03-01T16:18:57.720+0700\u001b[0m] {\u001b[34m1582760252.py:\u001b[0m20} INFO\u001b[0m - Adding JARs to Spark session: /Users/sawitpro/Documents/snippets/porto/etl_project/driver/clickhouse-jdbc-0.4.6.jar,/Users/sawitpro/Documents/snippets/porto/etl_project/driver/postgresql-42.7.5.jar\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-03-01T16:19:01.773+0700\u001b[0m] {\u001b[34m3267859770.py:\u001b[0m43} INFO\u001b[0m - Transformed data succesfully saved to: transformed/2025-03-01.daily_transaction_summary.parquet\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/01 16:19:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "transform(table_source = table_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow Variables for parameterization\n",
    "AIRFLOW_PATH = Variable.get(\"LOCAL_AIRFLOW_PATH\")\n",
    "POSTGRES_JDBC_URL = Variable.get(\"POSTGRES_JDBC_URL\")\n",
    "POSTGRES_PASSWORD = Variable.get(\"POSTGRES_PASSWORD\")\n",
    "CLIKCHOUSE_CONN_ID = \"clickhouse\"\n",
    "POSTGRES_SCHEMA = \"public\"\n",
    "CLICKHOUSE_SCHEMA = \"default\"\n",
    "TABLENAME = \"person_detail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-03-01T17:41:59.054+0700\u001b[0m] {\u001b[34m1582760252.py:\u001b[0m20} INFO\u001b[0m - Adding JARs to Spark session: /Users/sawitpro/Documents/snippets/porto/etl_project/driver/clickhouse-jdbc-0.4.6.jar,/Users/sawitpro/Documents/snippets/porto/etl_project/driver/postgresql-42.7.5.jar\u001b[0m\n",
      "[\u001b[34m2025-03-01T17:41:59.766+0700\u001b[0m] {\u001b[34m3900943316.py:\u001b[0m66} INFO\u001b[0m - Data successfully loaded into ClickHouse table: daily_transaction_summary\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def load(sink_tablename : str):\n",
    "    \"\"\"\n",
    "    Load transformed data from Parquet into ClickHouse database using JDBC.\n",
    "    \"\"\"\n",
    "    import clickhouse_connect\n",
    "    spark = create_spark_session(\"Load-to-ClickHouse\")\n",
    "    ch_client = clickhouse_connect.get_client(\n",
    "        host = \"127.0.0.1\",\n",
    "        port = 8123,\n",
    "        username = \"spark\",\n",
    "        password = Variable.get(\"CLICKHOUSE_PASSWORD\"))\n",
    "\n",
    "    stg_table = f\"`intermediate`.{sink_tablename}_stg\"\n",
    "    ch_stg_ddl = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {stg_table} (\n",
    "        transaction_date Date,\n",
    "        item_category String,\n",
    "        total_transaction_value Float64,\n",
    "        total_goods_sold Int32,\n",
    "        count_transacting_customer Int32,\n",
    "        updated_at UInt32 DEFAULT toUnixTimestamp(now())\n",
    "    )\n",
    "    ENGINE = MergeTree()\n",
    "    ORDER BY (transaction_date, item_category)\n",
    "    \"\"\"\n",
    "    ch_client.query(ch_stg_ddl)\n",
    "\n",
    "    prod_table = f\"`default`.{sink_tablename}\"\n",
    "    ch_prod_ddl = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {prod_table} (\n",
    "        transaction_date Date,\n",
    "        item_category String,\n",
    "        total_transaction_value Float64,\n",
    "        total_goods_sold Int32,\n",
    "        count_transacting_customer Int32,\n",
    "        updated_at UInt32 DEFAULT toUnixTimestamp(now())\n",
    "    )\n",
    "    ENGINE = ReplacingMergeTree(updated_at)\n",
    "    ORDER BY (transaction_date, item_category);\n",
    "    \"\"\"\n",
    "    ch_client.query(ch_prod_ddl)\n",
    "\n",
    "    transformed_path = f\"transformed/{current_date_str}.{sink_tablename}.parquet\"\n",
    "    transformed_df = spark.read.parquet(transformed_path)\n",
    "\n",
    "    transformed_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:clickhouse://127.0.0.1:8123/default\") \\\n",
    "        .option(\"dbtable\", stg_table) \\\n",
    "        .option(\"user\", \"spark\") \\\n",
    "        .option(\"password\", \"spark\") \\\n",
    "        .option(\"driver\", \"com.clickhouse.jdbc.ClickHouseDriver\") \\\n",
    "        .option(\"batchsize\", \"50000\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    \n",
    "    ch_prod_dml = f\"\"\"\n",
    "    INSERT INTO {sink_tablename}\n",
    "    SELECT * FROM {stg_table}\n",
    "    \"\"\"\n",
    "    ch_client.query(ch_prod_dml)\n",
    "    ch_prod_merge = f\"\"\"\n",
    "    OPTIMIZE TABLE {prod_table} FINAL\n",
    "    \"\"\"\n",
    "    ch_client.query(ch_prod_merge)\n",
    "    logger.info(f\"Data successfully loaded into ClickHouse table: {sink_tablename}\")\n",
    "\n",
    "load(\"daily_transaction_summary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
